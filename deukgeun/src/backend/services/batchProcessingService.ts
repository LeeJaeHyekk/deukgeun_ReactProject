import { Repository } from "typeorm"
import { Gym } from "../entities/Gym.js"
import { logger } from "../utils/logger.js"
import { ErrorHandlingService, ErrorContext } from "./errorHandlingService.js"

export interface BatchConfig {
  batchSize: number
  concurrency: number
  delayBetweenBatches: number
  maxRetries: number
  timeout: number
}

export interface BatchProgress {
  total: number
  processed: number
  success: number
  failed: number
  currentBatch: number
  totalBatches: number
  estimatedTimeRemaining: number
  startTime: Date
  lastUpdateTime: Date
}

export interface BatchResult<T> {
  success: boolean
  data: T[]
  errors: Error[]
  progress: BatchProgress
  duration: number
}

export class BatchProcessingService {
  private static readonly DEFAULT_CONFIG: BatchConfig = {
    batchSize: 10,
    concurrency: 3,
    delayBetweenBatches: 1000,
    maxRetries: 3,
    timeout: 30000
  }

  /**
   * ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ í—¬ìŠ¤ì¥ ë°ì´í„° ì—…ë°ì´íŠ¸
   */
  static async processGymsInBatches(
    gymRepo: Repository<Gym>,
    updateFunction: (gym: Gym) => Promise<any>,
    config: Partial<BatchConfig> = {}
  ): Promise<BatchResult<any>> {
    const finalConfig = { ...this.DEFAULT_CONFIG, ...config }
    const allGyms = await gymRepo.find()
    
    logger.info(`ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ${allGyms.length}ê°œ í—¬ìŠ¤ì¥, ë°°ì¹˜ í¬ê¸°: ${finalConfig.batchSize}`)
    
    const progress: BatchProgress = {
      total: allGyms.length,
      processed: 0,
      success: 0,
      failed: 0,
      currentBatch: 0,
      totalBatches: Math.ceil(allGyms.length / finalConfig.batchSize),
      estimatedTimeRemaining: 0,
      startTime: new Date(),
      lastUpdateTime: new Date()
    }

    const results: any[] = []
    const errors: Error[] = []

    // ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    const batches = this.createBatches(allGyms, finalConfig.batchSize)
    
    for (let i = 0; i < batches.length; i++) {
      progress.currentBatch = i + 1
      const batch = batches[i]
      
      logger.info(`ğŸ“¦ ë°°ì¹˜ ${i + 1}/${batches.length} ì²˜ë¦¬ ì¤‘: ${batch.length}ê°œ í—¬ìŠ¤ì¥`)
      
      try {
        // ë°°ì¹˜ ë‚´ì—ì„œ ë™ì‹œ ì²˜ë¦¬
        const batchResults = await this.processBatchConcurrently(
          batch,
          updateFunction,
          finalConfig,
          progress
        )
        
        results.push(...batchResults.successful)
        errors.push(...batchResults.errors)
        
        progress.success += batchResults.successful.length
        progress.failed += batchResults.errors.length
        progress.processed += batch.length
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        this.updateProgress(progress)
        
        // ë°°ì¹˜ ê°„ ì§€ì—°
        if (i < batches.length - 1) {
          await new Promise(resolve => setTimeout(resolve, finalConfig.delayBetweenBatches))
        }
        
      } catch (error) {
        logger.error(`âŒ ë°°ì¹˜ ${i + 1} ì²˜ë¦¬ ì‹¤íŒ¨:`, error)
        errors.push(error as Error)
        progress.failed += batch.length
        progress.processed += batch.length
      }
    }

    const duration = Date.now() - progress.startTime.getTime()
    
    logger.info(`âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ${progress.success}ê°œ ì„±ê³µ, ${progress.failed}ê°œ ì‹¤íŒ¨, ${duration}ms ì†Œìš”`)
    
    return {
      success: progress.failed === 0,
      data: results,
      errors,
      progress,
      duration
    }
  }

  /**
   * ë°°ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬
   */
  private static async processBatchConcurrently<T>(
    batch: T[],
    processFunction: (item: T) => Promise<any>,
    config: BatchConfig,
    progress: BatchProgress
  ): Promise<{
    successful: any[]
    errors: Error[]
  }> {
    const successful: any[] = []
    const errors: Error[] = []
    
    // ë™ì‹œ ì²˜ë¦¬ ì œí•œ
    const semaphore = new Semaphore(config.concurrency)
    
    const promises = batch.map(async (item) => {
      await semaphore.acquire()
      
      try {
        const result = await this.processWithRetry(item, processFunction, config)
        successful.push(result)
        return result
      } catch (error) {
        errors.push(error as Error)
        throw error
      } finally {
        semaphore.release()
      }
    })
    
    await Promise.allSettled(promises)
    
    return { successful, errors }
  }

  /**
   * ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ê°œë³„ ì²˜ë¦¬
   */
  private static async processWithRetry<T>(
    item: T,
    processFunction: (item: T) => Promise<any>,
    config: BatchConfig
  ): Promise<any> {
    const context: ErrorContext = {
      gymName: (item as any).name || "unknown",
      source: "batch_processing",
      error: new Error("Initial error"),
      timestamp: new Date(),
      retryCount: 0
    }
    
    return ErrorHandlingService.retryWithBackoff(
      async () => {
        const timeoutPromise = new Promise((_, reject) => {
          setTimeout(() => reject(new Error("Timeout")), config.timeout)
        })
        
        const processPromise = processFunction(item)
        
        return Promise.race([processPromise, timeoutPromise])
      },
      context,
      config.maxRetries
    )
  }

  /**
   * ë°°ì—´ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
   */
  private static createBatches<T>(array: T[], batchSize: number): T[][] {
    const batches: T[][] = []
    
    for (let i = 0; i < array.length; i += batchSize) {
      batches.push(array.slice(i, i + batchSize))
    }
    
    return batches
  }

  /**
   * ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
   */
  private static updateProgress(progress: BatchProgress): void {
    const now = new Date()
    const elapsed = now.getTime() - progress.startTime.getTime()
    
    if (progress.processed > 0) {
      const avgTimePerItem = elapsed / progress.processed
      const remainingItems = progress.total - progress.processed
      progress.estimatedTimeRemaining = avgTimePerItem * remainingItems
    }
    
    progress.lastUpdateTime = now
    
    // ì§„í–‰ë¥  ë¡œê¹…
    const percentage = ((progress.processed / progress.total) * 100).toFixed(1)
    logger.info(`ğŸ“Š ì§„í–‰ë¥ : ${percentage}% (${progress.processed}/${progress.total}) - ì„±ê³µ: ${progress.success}, ì‹¤íŒ¨: ${progress.failed}`)
  }

  /**
   * ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
   */
  static async processWithMemoryOptimization<T>(
    items: T[],
    processFunction: (item: T) => Promise<any>,
    config: Partial<BatchConfig> = {}
  ): Promise<BatchResult<any>> {
    const finalConfig = { ...this.DEFAULT_CONFIG, ...config }
    
    // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
    const startMemory = process.memoryUsage()
    logger.info(`ğŸ’¾ ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ${this.formatBytes(startMemory.heapUsed)}`)
    
    const results: any[] = []
    const errors: Error[] = []
    let processed = 0
    
    // ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
    for (let i = 0; i < items.length; i += finalConfig.batchSize) {
      const batch = items.slice(i, i + finalConfig.batchSize)
      
      try {
        const batchResults = await this.processBatchConcurrently(
          batch,
          processFunction,
          finalConfig,
          {
            total: items.length,
            processed,
            success: results.length,
            failed: errors.length,
            currentBatch: Math.floor(i / finalConfig.batchSize) + 1,
            totalBatches: Math.ceil(items.length / finalConfig.batchSize),
            estimatedTimeRemaining: 0,
            startTime: new Date(),
            lastUpdateTime: new Date()
          }
        )
        
        results.push(...batchResults.successful)
        errors.push(...batchResults.errors)
        processed += batch.length
        
        // ë©”ëª¨ë¦¬ ì •ë¦¬
        if (global.gc) {
          global.gc()
        }
        
        // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
        const currentMemory = process.memoryUsage()
        logger.info(`ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ${this.formatBytes(currentMemory.heapUsed)}`)
        
      } catch (error) {
        logger.error(`âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨:`, error)
        errors.push(error as Error)
        processed += batch.length
      }
    }
    
    const endMemory = process.memoryUsage()
    logger.info(`ğŸ’¾ ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ${this.formatBytes(endMemory.heapUsed)}`)
    
    return {
      success: errors.length === 0,
      data: results,
      errors,
      progress: {
        total: items.length,
        processed,
        success: results.length,
        failed: errors.length,
        currentBatch: Math.ceil(items.length / finalConfig.batchSize),
        totalBatches: Math.ceil(items.length / finalConfig.batchSize),
        estimatedTimeRemaining: 0,
        startTime: new Date(),
        lastUpdateTime: new Date()
      },
      duration: 0
    }
  }

  /**
   * ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
   */
  static calculateOptimalBatchSize(totalItems: number, availableMemory: number): number {
    // ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
    const estimatedMemoryPerItem = 1024 * 1024 // 1MB per item (ì¶”ì •)
    const maxItemsByMemory = Math.floor(availableMemory / estimatedMemoryPerItem)
    
    // ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°ì™€ ë©”ëª¨ë¦¬ ì œí•œ ì¤‘ ì‘ì€ ê°’ ì„ íƒ
    const optimalSize = Math.min(this.DEFAULT_CONFIG.batchSize, maxItemsByMemory)
    
    // ìµœì†Œê°’ ë³´ì¥
    return Math.max(optimalSize, 1)
  }

  /**
   * ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´
   */
  private static formatBytes(bytes: number): string {
    const sizes = ['Bytes', 'KB', 'MB', 'GB']
    if (bytes === 0) return '0 Bytes'
    const i = Math.floor(Math.log(bytes) / Math.log(1024))
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i]
  }
}

/**
 * ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ í´ë˜ìŠ¤
 */
class Semaphore {
  private permits: number
  private waiting: Array<() => void> = []

  constructor(permits: number) {
    this.permits = permits
  }

  async acquire(): Promise<void> {
    if (this.permits > 0) {
      this.permits--
      return Promise.resolve()
    }

    return new Promise<void>((resolve) => {
      this.waiting.push(resolve)
    })
  }

  release(): void {
    if (this.waiting.length > 0) {
      const resolve = this.waiting.shift()!
      resolve()
    } else {
      this.permits++
    }
  }
}
